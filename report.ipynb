{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a593a5f-2960-4f5a-bd11-5a3c64dbc10e",
   "metadata": {},
   "source": [
    "# **BBM409 ASSIGNMENT 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588ab46-abba-48c4-b73d-4bcf3546062c",
   "metadata": {
    "tags": []
   },
   "source": [
    "        Eylül TUNCEL - 21727801\n",
    "        Emre KÖSEN   - 21727498"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0475570-80ec-4e8e-a156-d54d26f4bf4b",
   "metadata": {},
   "source": [
    "In this assignment, we will try to determine whether a mail is ham or spam with Naive Bayes classifier. Naive Bayes is a simple classification algorith that makes an assumption about the conditional independence of features. In our dataset, there are two columns (text, spam).\n",
    "* ”text”: the text of the article, could be incomplete\n",
    "* ”spam”: a label that marks the article as potentially spam or ham\n",
    "    - 1: spam\n",
    "    - 0: ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d0dd3-1c57-4548-80be-6cff452e2044",
   "metadata": {},
   "source": [
    "Below you can see implementation for all parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd3817d-6f9f-45c3-8c9b-5f80dffd9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b1f31e-97e8-4c06-8b03-0f67d0c3e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x):\n",
    "    # start and end points of each fold\n",
    "    size = int(x.shape[0] / 5)\n",
    "    # size = 100\n",
    "\n",
    "    # 1/5 part of the data set as test data\n",
    "    x_test = x[0:size]\n",
    "\n",
    "    # 4/5 part of the data set as test data\n",
    "    x_train = x[size:]\n",
    "    return x_test, x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89389a91-8cfd-4dde-8f87-becfb5486c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(x, n_gram):\n",
    "    # text_col refers to the first column which has mail texts\n",
    "    text_col = []\n",
    "    for i in range(x.shape[0]):\n",
    "        text_col.append(x[i, 0])\n",
    "\n",
    "    # initialize count vectorizer\n",
    "    if n_gram == 1:\n",
    "        count_vectorizer = CountVectorizer(max_df=1.0, min_df=1)\n",
    "    elif n_gram == 2:\n",
    "        count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    # matrix of token counts\n",
    "    matrix = count_vectorizer.fit_transform(text_col)\n",
    "\n",
    "    # vocabulary is the array of unique words which appears in all train texts\n",
    "    vocabulary = count_vectorizer.get_feature_names()\n",
    "    matrix = matrix.toarray()\n",
    "\n",
    "    # unique words dictionary is a dict which has all unique words as key and an array [spam count, ham count] as value\n",
    "    # one example key value pair is { \"word\" : [ 12, 30 ] ,  , }\n",
    "    unique_words_dict = dict.fromkeys(vocabulary)\n",
    "    for key in unique_words_dict:\n",
    "        # initialize spam and ham count as [0,0] at the beginning\n",
    "        unique_words_dict[key] = [0, 0]\n",
    "\n",
    "    count_spam_mails = 0\n",
    "    count_ham_mails = 0\n",
    "    total_spam_words = 0\n",
    "    total_ham_words = 0\n",
    "\n",
    "    # for all train samples\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        # if the sample is HAM\n",
    "        if x[i][1] == 0:\n",
    "            count_ham_mails += 1\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] != 0:\n",
    "                    # increase count by one in unique words dictionary\n",
    "                    w = vocabulary[j]\n",
    "                    unique_words_dict[w] = [unique_words_dict.get(w)[0], unique_words_dict.get(w)[1] + 1]\n",
    "                    total_ham_words += 1\n",
    "\n",
    "        # if the sample is SPAM\n",
    "        else:\n",
    "            count_spam_mails += 1\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] != 0:\n",
    "                    w = vocabulary[j]\n",
    "                    unique_words_dict[w] = [unique_words_dict.get(w)[0] + 1, unique_words_dict.get(w)[1]]\n",
    "                    total_spam_words += 1\n",
    "\n",
    "    # total mail count of spam and ham mails\n",
    "    count_mails = [count_spam_mails, count_ham_mails]\n",
    "\n",
    "    # total word count appeared in spam and ham mails\n",
    "    total_words_dist = [total_spam_words, total_ham_words]\n",
    "\n",
    "    return unique_words_dict, count_mails, total_words_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a786bf0-0eec-4f82-8426-5559c24db1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(x_test, unique_words_dict, total_words_dist, n_gram):\n",
    "    # take all the texts of mails in test data\n",
    "    text_col = []\n",
    "    for i in range(x_test.shape[0]):\n",
    "        text_col.append(x_test[i, 0])\n",
    "\n",
    "    # initialize count vectorizer\n",
    "    if n_gram == 1:\n",
    "        count_vectorizer = CountVectorizer(max_df=1.0, min_df=1)\n",
    "    elif n_gram == 2:\n",
    "        count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    # matrix of token counts in test data\n",
    "    matrix = count_vectorizer.fit_transform(text_col)\n",
    "    vocabulary = count_vectorizer.get_feature_names()\n",
    "    matrix = matrix.toarray()\n",
    "\n",
    "    # probability of being spam\n",
    "    prob_spam = total_words_dist[0] / (total_words_dist[0] + total_words_dist[1])\n",
    "    # probability of being ham\n",
    "    prob_ham = total_words_dist[1] / (total_words_dist[0] + total_words_dist[1])\n",
    "\n",
    "    predictions = []\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over all test samples\n",
    "    for i in range(x_test.shape[0]):\n",
    "\n",
    "        # take log of the probability of being spam mail and being ham mail\n",
    "        probability_spam = math.log2(prob_spam)\n",
    "        probability_ham = math.log2(prob_ham)\n",
    "\n",
    "        # Iterate over all words of one test sample\n",
    "        for j in range(len(matrix[i])):\n",
    "\n",
    "            # if matrix[i][j] is a number different than zero that means this word appears in that train sample\n",
    "            if matrix[i][j] != 0:\n",
    "\n",
    "                # Spam and ham count starts from 1 because of laplace smoothing\n",
    "                spam_count = 1\n",
    "                ham_count = 1\n",
    "\n",
    "                # We add number of unique words for laplace smoothing\n",
    "                spam_denominator = total_words_dist[0] + len(unique_words_dict)\n",
    "                ham_denominator = total_words_dist[1] + len(unique_words_dict)\n",
    "\n",
    "                word = unique_words_dict.get(vocabulary[j])\n",
    "\n",
    "                # Check whether word is in training samples or not\n",
    "                if word is not None:\n",
    "                    spam_count += word[0]\n",
    "                    ham_count += word[1]\n",
    "\n",
    "                # take log of the probabilities and sum them up\n",
    "                probability_spam += math.log2(spam_count / spam_denominator)\n",
    "                probability_ham += math.log2(ham_count / ham_denominator)\n",
    "\n",
    "\n",
    "        # by the naive bayes algorithm , take maximum probability as prediction class\n",
    "        if probability_spam > probability_ham:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "\n",
    "        # result array has two dimensional arrays in it for each test sample\n",
    "        # test sample x = [ actual class, predicted class ]\n",
    "        results[i] = [x_test[i][1], predictions[i]]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11bece8-8677-4a19-ac29-cc937895a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(x, stop_words_out):\n",
    "    # for tf_idf vectorizer, we first seperate the train data as ham an spam\n",
    "    # their respecttive values should be different for each word\n",
    "    text_col_spam = []\n",
    "    text_col_ham = []\n",
    "    for i in range(x.shape[0]):\n",
    "        if x[i][1] == 1:\n",
    "            text_col_spam.append(x[i, 0])\n",
    "        elif x[i][1] == 0:\n",
    "            text_col_ham.append(x[i, 0])\n",
    "\n",
    "    my_stop_words = ENGLISH_STOP_WORDS.union()\n",
    "\n",
    "    # for spam mails\n",
    "    count_vectorizer_spam = CountVectorizer()\n",
    "    matrix = count_vectorizer_spam.fit_transform(text_col_spam)\n",
    "    # get the vocabularies from spam mails\n",
    "    vocabulary_spam = count_vectorizer_spam.get_feature_names()\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_spam)), ('tfidf', TfidfTransformer())]).fit(\n",
    "        text_col_spam)\n",
    "\n",
    "    spam_tf_idf_arr = pipe['tfidf'].idf_\n",
    "    # make dictionary of all words with their respective tf_idf values\n",
    "    spam_tf_idf_dict = {}\n",
    "    for i in range(len(spam_tf_idf_arr)):\n",
    "        spam_tf_idf_dict[vocabulary_spam[i]] = spam_tf_idf_arr[i]\n",
    "\n",
    "    spam_tf_idf_arr = sorted(spam_tf_idf_arr)\n",
    "    spam_words = []\n",
    "    for i in range(100):\n",
    "        val = spam_tf_idf_arr[i]\n",
    "        for el in spam_tf_idf_dict.keys():\n",
    "            if spam_tf_idf_dict.get(el) == val:\n",
    "                spam_words.append(el)\n",
    "                break\n",
    "\n",
    "    # for ham mails\n",
    "    count_vectorizer_ham = CountVectorizer()\n",
    "    matrix = count_vectorizer_ham.fit_transform(text_col_ham)\n",
    "    # get the vocabularies from ham mails\n",
    "    vocabulary_ham = count_vectorizer_ham.get_feature_names()\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_ham)), ('tfidf', TfidfTransformer())]).fit(\n",
    "        text_col_ham)\n",
    "\n",
    "    ham_tf_idf_arr = pipe['tfidf'].idf_\n",
    "    # make dictionary of all words with their respective tf_idf values\n",
    "    ham_tf_idf_dict = {}\n",
    "    #create an \n",
    "    for i in range(len(ham_tf_idf_arr)):\n",
    "        ham_tf_idf_dict[vocabulary_ham[i]] = ham_tf_idf_arr[i]\n",
    "\n",
    "    ham_tf_idf_arr = sorted(ham_tf_idf_arr)\n",
    "    ham_words = []\n",
    "    for i in range(100):\n",
    "        val = ham_tf_idf_arr[i]\n",
    "        for el in ham_tf_idf_dict.keys():\n",
    "            if ham_tf_idf_dict.get(el) == val:\n",
    "                ham_words.append(el)\n",
    "                break\n",
    "\n",
    "    spam_words = list(dict.fromkeys(spam_words))\n",
    "    ham_words = list(dict.fromkeys(ham_words))\n",
    "    \n",
    "    # get union of spam and ham words as \"s\"\n",
    "    a, b = [spam_words, ham_words]\n",
    "    s = [x for x in b if x in a]\n",
    "    # if same words tf_idf value is too close in different classes \n",
    "    # that means their freqencies in those class are very similar, therefore not a indication of the class\n",
    "    # we remove those words\n",
    "    for i in range(len(s)):\n",
    "        word = s[i]\n",
    "        if (spam_tf_idf_dict.get(word) - ham_tf_idf_dict.get(word)) < 1.5:\n",
    "            if spam_words.count(word) > 0:\n",
    "                spam_words.remove(word)\n",
    "            if ham_words.count(word) > 0:\n",
    "                ham_words.remove(word)\n",
    "\n",
    "    # if we want to detect non-stop words\n",
    "    if stop_words_out:\n",
    "        sp = spam_words.copy()\n",
    "        for i in range(len(sp)):\n",
    "            word = sp[i]\n",
    "            # remove stop words which are in spam words\n",
    "            if len(my_stop_words.intersection([word])) > 0:\n",
    "                spam_words.remove(word)\n",
    "\n",
    "        hm = ham_words.copy()\n",
    "        for i in range(len(hm)):\n",
    "            word = hm[i]\n",
    "            # remove stop words which are in ham words\n",
    "            if len(my_stop_words.intersection([word])) > 0:\n",
    "                ham_words.remove(word)\n",
    "\n",
    "    print(\"Spam Words\", spam_words[:10])\n",
    "    print(\"Ham Words\", ham_words[:10])\n",
    "    print()\n",
    "\n",
    "    \n",
    "    # for reimplementing naive bayes with tf_idf vectorizer\n",
    "    total_spam_words_val = 0\n",
    "    total_ham_words_val = 0\n",
    "    # create a dictionar which contains all unique words as keys\n",
    "    # and their respective spam tf_idf values and ham tf_idf values as values \n",
    "    # word (key) -> [spam tf_idf , ham tf_idf]\n",
    "    unique_words_dict = {}\n",
    "    for el in spam_tf_idf_dict.keys():\n",
    "        x = spam_tf_idf_dict.get(el)\n",
    "        unique_words_dict[el] = [x, 0]\n",
    "        total_spam_words_val += spam_tf_idf_dict.get(el)\n",
    "    for el in ham_tf_idf_dict.keys():\n",
    "        x = ham_tf_idf_dict.get(el)\n",
    "        if unique_words_dict.get(el) is not None:\n",
    "            unique_words_dict[el] = [unique_words_dict.get(el)[0], x]\n",
    "        else:\n",
    "            unique_words_dict[el] = [0, ham_tf_idf_dict.get(el)]\n",
    "        total_ham_words_val += ham_tf_idf_dict.get(el)\n",
    "\n",
    "    total_words_dist = [(total_spam_words_val / len(spam_tf_idf_dict)), (total_ham_words_val/ len(ham_tf_idf_dict))]\n",
    "\n",
    "    dict_copy = unique_words_dict.copy()\n",
    "    # remove the  words with unspecific tf_idf values (too high or too low)\n",
    "    # too high value means the word occured very less (once or twice)\n",
    "    # too high values means the word occured too much so that doesnt't help to distinguish\n",
    "    for el in dict_copy.keys():\n",
    "        arr = unique_words_dict.get(el)\n",
    "        if arr[0] > 6 or arr[0] < 1.5:\n",
    "            unique_words_dict[el] = [0, unique_words_dict.get(el)[1]]\n",
    "        if arr[1] > 6 or arr[1] < 1.5:\n",
    "            unique_words_dict[el] = [unique_words_dict.get(el)[0], 0]\n",
    "\n",
    "    return unique_words_dict, total_words_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa622054-ac41-4424-98c5-562eaf41b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while calculating performance metrics\n",
    "# True positive = th -> truly predicted ham mail\n",
    "# False positive = fh -> falsely predicted ham mail\n",
    "# True negative = ts -> truly predicted spam mail\n",
    "# False negative = fs -> falsely predicted spam mail\n",
    "def calculate_performance(results):\n",
    "    th = 0\n",
    "    ts = 0\n",
    "    fh = 0\n",
    "    fs = 0\n",
    "    for key, value in results.items():\n",
    "        # if the mail is ham and predicted as ham\n",
    "        if value[0] == value[1] and value[1] == 0:\n",
    "            th += 1\n",
    "        # if the mail is spam and predicted as spam\n",
    "        elif value[0] == value[1] and value[1] == 1:\n",
    "            ts += 1\n",
    "        # if the mail is spam but predicted as ham\n",
    "        if value[0] != value[1] and value[1] == 0:\n",
    "            fh += 1\n",
    "        # if the mail is ham but predicted as spam\n",
    "        elif value[0] != value[1] and value[1] == 1:\n",
    "            fs += 1\n",
    "\n",
    "    accuracy = (th + ts) / (th + ts + fh + fs)\n",
    "    precision = th / (th + fh)\n",
    "    recall = th / (th + fs)\n",
    "    f1_score = (2 * recall * precision) / (recall + precision)\n",
    "    return accuracy, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2929973a-076b-437d-b1a1-c4d2f86a2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function gets most frequent words which are in spam/ham mails\n",
    "# but the words appear both in spam and ham mails are didn't accepted\n",
    "def max_prob_words_by_naive_bayes(unique_words_dict, total_words_dist):\n",
    "    max_spam_count = 0\n",
    "    max_spam_word1 = \"\"\n",
    "    max_spam_word2 = \"\"\n",
    "    max_spam_word3 = \"\"\n",
    "    max_ham_count = 0\n",
    "    max_ham_word1 = \"\"\n",
    "    max_ham_word2 = \"\"\n",
    "    max_ham_word3 = \"\"\n",
    "\n",
    "    for x in unique_words_dict.keys():\n",
    "        arr = unique_words_dict.get(x)\n",
    "\n",
    "        # spam\n",
    "        if arr[0] > max_spam_count and ((arr[1] / total_words_dist[1]) / (arr[0] / total_words_dist[0])) < 0.5:\n",
    "            max_spam_count = arr[0]\n",
    "            max_spam_word3 = max_spam_word2\n",
    "            max_spam_word2 = max_spam_word1\n",
    "            max_spam_word1 = x\n",
    "\n",
    "        # ham\n",
    "        if arr[1] > max_ham_count and ((arr[0] / total_words_dist[0]) / (arr[1] / total_words_dist[1])) < 0.3:\n",
    "            max_ham_count = arr[1]\n",
    "            max_ham_word3 = max_ham_word2\n",
    "            max_ham_word2 = max_ham_word1\n",
    "            max_ham_word1 = x\n",
    "\n",
    "    print(\"Most frequent ham word  : \", max_ham_word1)\n",
    "    print(\"Second frequent ham word : \", max_ham_word2)\n",
    "    print(\"Third frequent ham word : \", max_ham_word3)\n",
    "    print()\n",
    "    print(\"Most frequent spam word : \", max_spam_word1)\n",
    "    print(\"Second frequent spam word : \", max_spam_word2)\n",
    "    print(\"Third frequent ham word : \", max_spam_word3)\n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112effe6-ae8e-4d6a-a5fc-596b4095e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data's in the csv file to the numpy array\n",
    "df = pd.read_csv('./emails.csv')\n",
    "x = np.array(df.iloc[:, :])\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(x)\n",
    "np.random.seed(102)\n",
    "np.random.shuffle(x)\n",
    "np.random.seed(103)\n",
    "np.random.shuffle(x)\n",
    "\n",
    "# split data %80 train - %20 test\n",
    "x_test, x_train = split_data(x.copy())\n",
    "\n",
    "# create dictionary of unique words\n",
    "unique_words_dict, count_mails, total_words_dist = vectorizer(x_train.copy(), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff73f0c-1117-4e0c-a545-428f6c5ea8b4",
   "metadata": {},
   "source": [
    "# **Part 1:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886c21de-0c65-47bb-8d9c-a7833880baba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART1 \n",
      "------------------------------------------------------------\n",
      "Most frequent ham word  :  vince\n",
      "Second frequent ham word :  enron\n",
      "Third frequent ham word :  cc\n",
      "\n",
      "Most frequent spam word :  here\n",
      "Second frequent spam word :  click\n",
      "Third frequent ham word :  000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PART1\n",
    "print(\"PART1 \\n------------------------------------------------------------\")\n",
    "max_prob_words_by_naive_bayes(unique_words_dict, total_words_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87919dbe-a838-4273-b815-150f72a8fcdf",
   "metadata": {},
   "source": [
    "## **1. Part 1: Understanding the data**\n",
    "\n",
    "\n",
    "For this part we implement a function named \"max_prob_words_by_naive_bayes\" (you can see in above code). In this function, we only consider frequencies of each word appeared in mails. Each words frequency calculated by count vectorizer whic is predefined in scikit learn library. By this vector we can calculate each words frequency in spam/ham mails. We calculate the frequencies for each word and make a dictionary with key value pair is shown as { word = [ spam count, ham count ] }. With those values we try to predict most usful keywords. When we try to figure out those keywords from our data, we try to select words which are not appeared often in both of the classes. If a word is appeared often in both classes, then this word has no effect on our prediction. \n",
    "\n",
    "\n",
    "\n",
    "With these values, we find 3 words which might be useful for us while predicting.\n",
    "\n",
    "Ham Mails:\n",
    "\n",
    "\n",
    "| | Word | Statistics |\n",
    "| --- | --- | --- |\n",
    "| Most frequent ham word | **vince** | appeared 2248 times in ham mails |\n",
    "| Second frequent ham word | **enron** | appeared 2081 times in ham mails |\n",
    "| Third frequent ham word | **cc** | appeared 1765 times in ham mails |\n",
    "\n",
    "\n",
    "\n",
    "Spam Mails:\n",
    "\n",
    "\n",
    "\n",
    "| | Word | Statistics |\n",
    "| --- | --- | --- |\n",
    "| Most frequent spam word | **here** | appeared 403 times in spam mails |\n",
    "| Second frequent spam word | **click** | appeared 246 times in spam mails |\n",
    "| Third frequent spam word | **000** | appeared 137 times in spam mails |\n",
    "\n",
    "So, as we can see some words appeared more than the others. And this makes that specific word a better indicator of spam/ham mail. As we can see with our eyes too, those words appeared very often in their classes.For example the word \"vince\" appeared 2248 time in ham mails but there arent any appearence in spam mails. So that makes \"vince\" a very great indicator of being ham mail. By using those values, we can implement our naive bayes algorithm. As we can see from the words, naive bayes algorithm is appropriate and applicable. While some words indicate being ham, others could indicate being spam. With naive bayes algorithm we can calculate the probability of both cases and make comparison between them. We think naive bayes algorithm is applicable, by looking at the data.\n",
    "\n",
    "But there is bad side of naive bayes algorithm ehich cannot be detected by looking probabilities of words. Words together in mails creates a context. Naive Bayes algorithm didn't consider context of mails just looking total probabilities one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ba5b5-69f3-4698-aa98-258cfaaa35a1",
   "metadata": {},
   "source": [
    "# **PART 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf9a5bb5-6ab3-4ed8-b4bf-ca67f4e77b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART2 \n",
      "------------------------------------------------------------\n",
      "Unigram Accuracy:  0.9921397379912664\n",
      "Unigram Precision:  0.9964994165694282\n",
      "Unigram recall:  0.9930232558139535\n",
      "Unigram F1 score:  0.9947582993593477\n",
      "\n",
      "Bigram Accuracy:  0.97117903930131\n",
      "Bigram Precision:  0.9975932611311673\n",
      "Bigram recall:  0.963953488372093\n",
      "Bigram F1 score:  0.9804849201655825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PART2\n",
    "print(\"PART2 \\n------------------------------------------------------------\")\n",
    "\n",
    "#UNIGRAM\n",
    "\n",
    "# calculate probabilities of all given test data\n",
    "results = naive_bayes(x_test.copy(), unique_words_dict, total_words_dist, 1)\n",
    "\n",
    "# calculate performance of the given results\n",
    "accuracy, precision, recall, f1_score = calculate_performance(results)\n",
    "print(\"Unigram Accuracy: \", accuracy)\n",
    "print(\"Unigram Precision: \", precision)\n",
    "print(\"Unigram recall: \", recall)\n",
    "print(\"Unigram F1 score: \", f1_score)\n",
    "print()\n",
    "\n",
    "\n",
    "# BIGRAM \n",
    "\n",
    "unique_words_dict, count_mails, total_words_dist = vectorizer(x_train.copy(), 2)\n",
    "results = naive_bayes(x_test.copy(), unique_words_dict, total_words_dist, 2)\n",
    "\n",
    "accuracy, precision, recall, f1_score = calculate_performance(results)\n",
    "print(\"Bigram Accuracy: \", accuracy)\n",
    "print(\"Bigram Precision: \", precision)\n",
    "print(\"Bigram recall: \", recall)\n",
    "print(\"Bigram F1 score: \", f1_score)\n",
    "print()"
   ]
  },
  {
   "attachments": {
    "874a8b48-0ef2-4688-b1bc-43a25643f9c3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "0288459a-0dd4-4a4a-a1c1-98d034e1f440",
   "metadata": {},
   "source": [
    "## **2. Part 2: Implementing Naive Bayes**\n",
    "\n",
    "In part2 we implemented Naive Bayes Algorithm. For this purpose we use CountVectorizer first. Count vectorizer creates a matrix which has frequencies of all words. In each row, there is another mail with frequencies of all unique words.\n",
    "When we implement Naive Bayes algorithm we use this formulation :\n",
    "\n",
    "![image.png](attachment:874a8b48-0ef2-4688-b1bc-43a25643f9c3.png)\n",
    "**_________________________________________________________________________________________________________**\n",
    "\n",
    "By using this formula we compute the log probabilities to prevent numerical underflow when calculating multiplicative probabilities.\n",
    "\n",
    "$ P(class|mail) = P(class) x P(word1|class) x P(word2|class) x ... x P(wordn|class) $\n",
    "\n",
    "\n",
    "$ log(P(class|mail)) = log( P(class) x P(word1|class) x P(word2|class) x ... x P(wordn|class) ) $\n",
    "\n",
    "\n",
    "$ log(P(class|mail)) = log(P(class)) + log(P(word1|class)) + log(P(word2|class)) + ... + log(P(wordn|class)) $\n",
    "\n",
    "\n",
    "**_________________________________________________________________________________________________________**\n",
    "\n",
    "\n",
    "\n",
    "And also use \"Laplace Smoothing\" because we may be encounter words during classification that you havent during training. We implement Laplace Smooothing as follows:\n",
    "\n",
    "\n",
    " $ Probability(word|class) = \\frac{FrequencyOfWord + 1}{TotalFrequencyOfClass + NumberOfUniqueWords} $ \n",
    "\n",
    "**_________________________________________________________________________________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc0b2e-eaa1-414a-8dd2-7edd4aed5631",
   "metadata": {},
   "source": [
    "And also we use Bag of Words (BoW) model which learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. This model is created by CountVectorizer. \n",
    "\n",
    "* **Unigram:**\n",
    "\n",
    "The occurrences of words in a document(frequency of the word).\n",
    "\n",
    "Count Vectorizer -> ngram_range=(1, 1)\n",
    "\n",
    "* **Bigram:** \n",
    "\n",
    "The occurrences of two adjacent words in a document.\n",
    "\n",
    "Count Vectorizer -> ngram_range=(2, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c584c41-0488-4c2e-b54d-156a3fae8027",
   "metadata": {},
   "source": [
    "## **3. Part 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7282879a-472a-40f8-8081-9b103bc5f0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART3 \n",
      "------------------------------------------------------------\n",
      "TF-IDF\n",
      "Spam Words ['here', 'no', 'http', 'business', 'email', 'click', 'best', 'make', 'money', 'only']\n",
      "Ham Words ['vince', 'enron', 'cc', 'me', 'am', 're', 'kaminski', 'would', 'thanks', '2000']\n",
      "\n",
      "Unigram Accuracy:  0.9510917030567686\n",
      "Unigram Precision:  0.9950738916256158\n",
      "Unigram recall:  0.9395348837209302\n",
      "Unigram F1 score:  0.9665071770334929\n",
      "\n",
      "NON STOP WORDS\n",
      "Spam Words ['http', 'business', 'email', 'click', 'best', 'make', 'money', 'free', 'life', 'offer']\n",
      "Ham Words ['vince', 'enron', 'cc', 'kaminski', 'thanks', '2000', 'pm', 'ect', 'hou', 'time']\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # PART3\n",
    "print(\"PART3 \\n------------------------------------------------------------\")\n",
    "print(\"TF-IDF\")\n",
    "unique_words_dict, total_words_dist = tf_idf(x_train.copy(), False)\n",
    "# calculate probabilities of all given test data\n",
    "results = naive_bayes(x_test.copy(), unique_words_dict, total_words_dist, 1)\n",
    "\n",
    "# calculate performance of the given results\n",
    "accuracy, precision, recall, f1_score = calculate_performance(results)\n",
    "print(\"Unigram Accuracy: \", accuracy)\n",
    "print(\"Unigram Precision: \", precision)\n",
    "print(\"Unigram recall: \", recall)\n",
    "print(\"Unigram F1 score: \", f1_score)\n",
    "print()\n",
    "\n",
    "print(\"NON STOP WORDS\")\n",
    "unique_words_dict, total_words_dist = tf_idf(x_train.copy(), True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02f3fd-b013-4bbb-9739-da4187d63362",
   "metadata": {},
   "source": [
    "### **3.a) Analyzing effect of the words on prediction**\n",
    "• **List the 10 words whose presence most strongly predicts that the mail is ham.**\n",
    "\n",
    "* vince\n",
    "* enron\n",
    "* cc\n",
    "* me\n",
    "* am\n",
    "* re\n",
    "* kaminski\n",
    "* would\n",
    "* thanks\n",
    "* 2000\n",
    "\n",
    "\n",
    "\n",
    "• **List the 10 words whose absence most strongly predicts that the mail is ham.**\n",
    "\n",
    "* here \n",
    "* no\n",
    "* http\n",
    "* business\n",
    "* email\n",
    "* click\n",
    "* best\n",
    "* make\n",
    "* money\n",
    "* only\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "• **List the 10 words whose presence most strongly predicts that the mail is spam.**\n",
    "\n",
    "* here \n",
    "* no\n",
    "* http\n",
    "* business\n",
    "* email\n",
    "* click\n",
    "* best\n",
    "* make\n",
    "* money\n",
    "* only\n",
    "\n",
    "\n",
    "\n",
    "• **List the 10 words whose absence most strongly predicts that the mail is spam**\n",
    "\n",
    "* vince\n",
    "* enron\n",
    "* cc\n",
    "* me\n",
    "* am\n",
    "* re\n",
    "* kaminski\n",
    "* would\n",
    "* thanks\n",
    "* 2000\n",
    "\n",
    "We used TF-IDF to extract the most frequent words in both spam and ham mails. First we split the spam and ham mails to process spam and ham related words seperately. The TF-IDF gives less score for if a word appears so much and more score if words less. A word can appear in both spam and ham mails so we need to eliminate these words. We eliminated the words that appear both in spam and ham mails with score difference less than 1.5. With this we are not dealing the words that appear in spam mails and also appear in ham mails.\n",
    "\n",
    "When we look to the outputs we see that when the words here, no, http, business, email, click, best, make, money and only appear in a mail that mail is mostly a spam mail. We can conclude that if presence of these words mostly predicts that mail is spam, absence of these words should predict that mail is ham. For ham mails the words that presence mostly predicts that mail is ham are vince, enron, cc, me, am, re, kaminski, would, thanks.\n",
    "\n",
    "At part4 you can see the accuracy, precision, recall and f1 score values of TF/IDF vectorizer on naive bayes classifier. We reimplement Naive Bayes with tf/idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c08e11-a5f6-4ada-ab8f-51cc9a79ba8d",
   "metadata": {},
   "source": [
    "### **3.b) Stopwords**\n",
    "\n",
    "• **List the 10 non-stopwords that most strongly predict that the mail is\n",
    "ham**\n",
    "* vince\n",
    "* enron\n",
    "* cc\n",
    "* kaminski\n",
    "* thanks\n",
    "* 2000\n",
    "* pm\n",
    "* ect\n",
    "* hou\n",
    "* time\n",
    "\n",
    "\n",
    "\n",
    "• **List 10 non-stopwords that most strongly predict that the mail is\n",
    "spam**\n",
    "* http\n",
    "* business\n",
    "* email\n",
    "* click\n",
    "* best\n",
    "* make\n",
    "* money\n",
    "* free\n",
    "* life\n",
    "* offer\n",
    "\n",
    "Stopwords are the words that ineffective when we process them in computers. We generally filter them before processing. Actually these words are the most common words in any language. Some examples in English are \"the\", \"a\", \"an\", \"so\", \"what\". These words don't add much information to the text they appear. So we can eliminate them before process mails. With scikit-learn library we have a list of English stopwords.\n",
    "\n",
    "After eliminating the stopwords our new top 10 words that mostly predicts that the mail is ham are vince, enron, cc, kaminski, 2000, pm, ect, hou, time. New top 10 words that mostly predicts that the mail is spam are http, business, email, click, best, make, money, free, life, offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd23ee-e66e-492e-b7f2-b46b77bae35f",
   "metadata": {},
   "source": [
    "### **3.c) Analyzing effect of the stopwords**\n",
    "\n",
    "#### Why might it make sense to remove stop words when interpreting the model?\n",
    "\n",
    "Stopwords are ineffective in the sentence that thay appear in general. If the stopwords don't add new information to sentence we can remove them. By removing them we are removing unnecessary words and the algorithm deals with the words that has more important information. Also these words might occur too much and be ineffective in probabilty calculations. We can improve computing performance by removing them.\n",
    "\n",
    "#### Why might it make sense to keep stop words?\n",
    "\n",
    "Deleting stopwords might affect the information or meaning of the sentence. For specific dataset some stopwords might be decisive when we classify mail is spam or ham. Deleting them might affect the algorithms performance.\n",
    "\n",
    "Conclusion is that we need to try both removing them and keeping them to measure performance of our model. Based on performance outputs we can remove or keep them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417305e-8a85-4956-93d2-de65e8be2686",
   "metadata": {},
   "source": [
    "## **4. Part 4: Calculation of Performance Metrics**\n",
    "You will compute performance metrics below of your model to measure the success\n",
    "of your classification method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6841708-f62e-46a4-ac07-2a46ed2df1e3",
   "metadata": {},
   "source": [
    "### **Unigram**\n",
    "##### max_df=1, min_df=1\n",
    "\n",
    "* Unigram Accuracy:  0.9921397379912664\n",
    "* Unigram Precision:  0.9964994165694282\n",
    "* Unigram recall:  0.9930232558139535\n",
    "* Unigram F1 score:  0.9947582993593477\n",
    "\n",
    "##### Time passed: 56.5368 seconds\n",
    "\n",
    "---\n",
    "\n",
    "##### max_df=0.85, min_df=0.01\n",
    "\n",
    "* Unigram Accuracy:  0.9676855895196507\n",
    "* Unigram Precision:  0.9963811821471653\n",
    "* Unigram recall:  0.9604651162790697\n",
    "* Unigram F1 score:  0.9780935464772055\n",
    "\n",
    "##### Time passed: 4.8810 seconds\n",
    "\n",
    "---\n",
    "\n",
    "##### max_df=0.90 min_df=0.005\n",
    "\n",
    "* Unigram Accuracy:  0.9764192139737992\n",
    "* Unigram Precision:  0.997610513739546\n",
    "* Unigram recall:  0.9709302325581395\n",
    "* Unigram F1 score:  0.9840895698291102\n",
    "\n",
    "##### Time passed: 6.9668 seconds\n",
    "\n",
    "---\n",
    "\n",
    "##### max_df=0.95 min_df=0.005\n",
    "\n",
    "* Unigram Accuracy:  0.9764192139737992\n",
    "* Unigram Precision:  0.997610513739546\n",
    "* Unigram recall:  0.9709302325581395\n",
    "* Unigram F1 score:  0.9840895698291102\n",
    "\n",
    "##### Time passed: 7.2592 seconds\n",
    "\n",
    "When using CountVectorizer to create a vocabulary of words we can set min_df and max_df values. Min_df value is a cut-off. For better explanation when building vocabulary the function ignores the terms that frequency is lower than the value. Because of very low frequency these words might be ineffective. Max_df is opposite of the min_df. It ignores the terms that frequency is higher than the value. These words might be stop-words and we can get rid of them.\n",
    "\n",
    "We get best performance with max_df=1 and min_df=1 values but with time trade-off. The running time of the algorithm is high. We can change the max_df and min_df values to shrink our vocabulary and work on smaller set of words. This improves the running time performance but disimproves the accuracy and F1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebbbe5-ee58-45d3-a2ed-1e2252e90166",
   "metadata": {},
   "source": [
    "### **Bigram**\n",
    "\n",
    "##### max_df=1, min_df=1\n",
    "\n",
    "* Bigram Accuracy:  0.97117903930131\n",
    "* Bigram Precision:  0.9975932611311673\n",
    "* Bigram recall:  0.963953488372093\n",
    "* Bigram F1 score:  0.9804849201655825\n",
    "\n",
    "##### Time passed: 471.2127\n",
    "\n",
    "---\n",
    "\n",
    "##### max_df=0.85, min_df=0.01\n",
    "\n",
    "* Bigram Accuracy:  0.9021834061135371\n",
    "* Bigram Precision:  0.9770408163265306\n",
    "* Bigram recall:  0.8906976744186047\n",
    "* Bigram F1 score:  0.9318734793187348\n",
    "\n",
    "##### Time passed: 4.3780\n",
    "\n",
    "---\n",
    "\n",
    "##### max_df=0.90, min_df=0.005\n",
    "\n",
    "* Bigram Accuracy:  0.9266375545851528\n",
    "* Bigram Precision:  0.9790123456790123\n",
    "* Bigram recall:  0.922093023255814\n",
    "* Bigram F1 score:  0.9497005988023952\n",
    "\n",
    "##### Time passed: 9.6360\n",
    "\n",
    "---\n",
    "\n",
    "##### max_df=0.95, min_df=0.005\n",
    "\n",
    "* Bigram Accuracy:  0.9266375545851528\n",
    "* Bigram Precision:  0.9790123456790123\n",
    "* Bigram recall:  0.922093023255814\n",
    "* Bigram F1 score:  0.9497005988023952\n",
    "\n",
    "##### Time passed: 9.6279\n",
    "\n",
    "We explanied the min_df and max_df values in the Unigram part. With Bigram we again get best performance with min_df=1 and max_df=1 values but running time is too high. We can shrink the vocabulary and gain big improvement in time but we lost accuracy around %6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075e397-bfcb-4118-ad18-e5ea73df0ad8",
   "metadata": {},
   "source": [
    "### **TF-IDF Vectorizer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffee5b5-836a-45ad-a87f-7efe9f3a087f",
   "metadata": {},
   "source": [
    "* Unigram Accuracy:  0.9510917030567686\n",
    "* Unigram Precision:  0.9950738916256158\n",
    "* Unigram recall:  0.9395348837209302\n",
    "* Unigram F1 score:  0.9665071770334929\n",
    "\n",
    "The performance values you see above is obtained by TF/IDF vectorizer on Naive Bayes Implementation. WHat we do to reimplement is firstly we get tf idf values for each word in spam/ham mails. Getting those values of specific class gives us hints about prediction. The minimum valued word is always \"subject\" because it is the most frequent one in mails. While tf idf value goes higher, the frequnency of the word is decreasing. The maximum valued words are the ones which are only encountered once in mails. For Naive Bayes implementation we get those values in the dictionary, seperated with spam and ham mails. The minmum valued words are not specific enough to make predictions (like \"subject\") so we drop those words. And also maximum valued words which are encoountered only once, they are not special enough to make predictions. Their frequency is very low and that means it doesn't effect the mail is being spam or ham. The tf_idf values ranges between 1 to 8. BUt we only take the words between 1.5 and 6 because that way our accuracy will increase and we make better prediction with more specified values.\n",
    "\n",
    "Above you can see accuracy, precision, recall and f1 score of TF-IDF Vectorizer. As you can see the values are very high. They range between 93 to 99. TF-IDF Vectorizer's performance is very high we think, but it's still a little bit behind the Count Vectorizer. While TF-IDF Vectorizer is a great way to detect mails, it is not the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c030662-0c0f-488e-acc5-cee10efc50fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}