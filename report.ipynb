{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a593a5f-2960-4f5a-bd11-5a3c64dbc10e",
   "metadata": {},
   "source": [
    "# **BBM409 ASSIGNMENT 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588ab46-abba-48c4-b73d-4bcf3546062c",
   "metadata": {},
   "source": [
    "        Eylül TUNCEL - 21727801\n",
    "        Emre KÖSEN   - 21727498"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0475570-80ec-4e8e-a156-d54d26f4bf4b",
   "metadata": {},
   "source": [
    "In this assignment, we will try to determine whether a mail is ham or spam with Naive Bayes classifier. Naive Bayes is a simple classification algorith that makes an assumption about the conditional independence of features. In our dataset, there are two columns (text, spam).\n",
    "* ”text”: the text of the article, could be incomplete\n",
    "* ”spam”: a label that marks the article as potentially spam or ham\n",
    "    - 1: spam\n",
    "    - 0: ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cec2c4-c93c-49f2-800e-45d43573d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c3b8e4-0919-4ab3-81e6-0325f2b5047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x):\n",
    "    # start and end points of each fold\n",
    "    size = int(x.shape[0] / 5)\n",
    "    # size = 100\n",
    "\n",
    "    # 1/5 part of the data set as test data\n",
    "    x_test = x[0:size]\n",
    "    print(x_test.shape)\n",
    "\n",
    "    # 4/5 part of the data set as test data\n",
    "    x_train = x[size:]\n",
    "    print(x_train.shape)\n",
    "    return x_test, x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76077f-0dd8-4262-b4ee-b2d95a69ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(x, n_gram):\n",
    "    # text_col refers to the first column which has mail texts\n",
    "    text_col = []\n",
    "    for i in range(x.shape[0]):\n",
    "        text_col.append(x[i, 0])\n",
    "\n",
    "    # initialize count vectorizer\n",
    "    if n_gram == 1:\n",
    "        count_vectorizer = CountVectorizer()\n",
    "    elif n_gram == 2:\n",
    "        count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    # matrix of token counts\n",
    "    matrix = count_vectorizer.fit_transform(text_col)\n",
    "\n",
    "    # vocabulary is the array of unique words which appears in all train texts\n",
    "    vocabulary = count_vectorizer.get_feature_names_out()\n",
    "    matrix = matrix.toarray()\n",
    "\n",
    "    # unique words dictionary is a dict which has all unique words as key and an array [spam count, ham count] as value\n",
    "    # one example key value pair is { \"word\" : [ 12, 30 ] ,  , }\n",
    "    unique_words_dict = dict.fromkeys(vocabulary)\n",
    "    for key in unique_words_dict:\n",
    "        # initialize spam and ham count as [0,0] at the beginning\n",
    "        unique_words_dict[key] = [0, 0]\n",
    "\n",
    "    count_spam_mails = 0\n",
    "    count_ham_mails = 0\n",
    "    total_spam_words = 0\n",
    "    total_ham_words = 0\n",
    "\n",
    "    # for all train samples\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        # if the sample is HAM\n",
    "        if x[i][1] == 0:\n",
    "            count_ham_mails += 1\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] != 0:\n",
    "                    # increase count by one in unique words dictionary\n",
    "                    w = vocabulary[j]\n",
    "                    unique_words_dict[w] = [unique_words_dict.get(w)[0], unique_words_dict.get(w)[1] + 1]\n",
    "                    total_ham_words += 1\n",
    "\n",
    "        # if the sample is SPAM\n",
    "        else:\n",
    "            count_spam_mails += 1\n",
    "            for j in range(len(matrix[i])):\n",
    "                if matrix[i][j] != 0:\n",
    "                    w = vocabulary[j]\n",
    "                    unique_words_dict[w] = [unique_words_dict.get(w)[0] + 1, unique_words_dict.get(w)[1]]\n",
    "                    total_spam_words += 1\n",
    "\n",
    "    # total mail count of spam and ham mails\n",
    "    count_mails = [count_spam_mails, count_ham_mails]\n",
    "\n",
    "    # total word count appeared in spam and ham mails\n",
    "    total_words_dist = [total_spam_words, total_ham_words]\n",
    "\n",
    "    return unique_words_dict, count_mails, total_words_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228c70c-ae46-4131-bd66-eee7a14d3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_prob_words_by_naive_bayes(unique_words_dict, total_words_dist):\n",
    "    max_spam_count = 0\n",
    "    max_spam_word1 = \"\"\n",
    "    max_spam_word2 = \"\"\n",
    "    max_spam_word3 = \"\"\n",
    "    max_ham_count = 0\n",
    "    max_ham_word1 = \"\"\n",
    "    max_ham_word2 = \"\"\n",
    "    max_ham_word3 = \"\"\n",
    "\n",
    "    for x in unique_words_dict.keys():\n",
    "        arr = unique_words_dict.get(x)\n",
    "\n",
    "        # spam\n",
    "        if arr[0] > max_spam_count and ((arr[1] / total_words_dist[1]) / (arr[0] / total_words_dist[0])) < 0.5:\n",
    "            max_spam_count = arr[0]\n",
    "            max_spam_word3 = max_spam_word2\n",
    "            max_spam_word2 = max_spam_word1\n",
    "            max_spam_word1 = x\n",
    "\n",
    "        # ham\n",
    "        if arr[1] > max_ham_count and ((arr[0] / total_words_dist[0]) / (arr[1] / total_words_dist[1])) < 0.3:\n",
    "            max_ham_count = arr[1]\n",
    "            max_ham_word3 = max_ham_word2\n",
    "            max_ham_word2 = max_ham_word1\n",
    "            max_ham_word1 = x\n",
    "\n",
    "    print(\"max ham1 : \", max_ham_word1)\n",
    "    print(\"max ham2 : \", max_ham_word2)\n",
    "    print(\"max ham3 : \", max_ham_word3)\n",
    "    print()\n",
    "    print(\"max spam1 : \", max_spam_word1)\n",
    "    print(\"max spam2 : \", max_spam_word2)\n",
    "    print(\"max spam3 : \", max_spam_word3)\n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ce52e-ec7d-4224-a2c9-3f75461659da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probability(x_test, unique_words_dict, count_mails, total_words_dist, n_gram):\n",
    "    # take all the texts of mails in test data\n",
    "    text_col = []\n",
    "    for i in range(x_test.shape[0]):\n",
    "        text_col.append(x_test[i, 0])\n",
    "\n",
    "    # initialize count vectorizer\n",
    "    if n_gram == 1:\n",
    "        count_vectorizer = CountVectorizer()\n",
    "    elif n_gram == 2:\n",
    "        count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    # matrix of token counts in test data\n",
    "    matrix = count_vectorizer.fit_transform(text_col)\n",
    "    vocabulary = count_vectorizer.get_feature_names_out()\n",
    "    matrix = matrix.toarray()\n",
    "\n",
    "    # probability of being spam\n",
    "    prob_spam = total_words_dist[0] / (total_words_dist[0] + total_words_dist[1])\n",
    "    # probability of being ham\n",
    "    prob_ham = total_words_dist[1] / (total_words_dist[0] + total_words_dist[1])\n",
    "\n",
    "    predictions = []\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over all test samples\n",
    "    for i in range(x_test.shape[0]):\n",
    "\n",
    "        # take log of the probability of being spam mail and being ham mail\n",
    "        probability_spam = math.log2(prob_spam)\n",
    "        probability_ham = math.log2(prob_ham)\n",
    "\n",
    "        # Iterate over all words of one test sample\n",
    "        for j in range(len(matrix[i])):\n",
    "\n",
    "            # if matrix[i][j] is a number different than zero that means this word appears in that train sample\n",
    "            if matrix[i][j] != 0:\n",
    "\n",
    "                # Spam and ham count starts from 1 because of laplace smoothing\n",
    "                spam_count = 1\n",
    "                ham_count = 1\n",
    "\n",
    "                # We add number of unique words for laplace smoothing\n",
    "                spam_denominator = total_words_dist[0] + len(unique_words_dict)\n",
    "                ham_denominator = total_words_dist[1] + len(unique_words_dict)\n",
    "\n",
    "                word = unique_words_dict.get(vocabulary[j])\n",
    "\n",
    "                # Check whether word is in training samples or not\n",
    "                if word is not None:\n",
    "                    spam_count += word[0]\n",
    "                    ham_count += word[1]\n",
    "\n",
    "                # take log of the probabilities and sum them up\n",
    "                probability_spam += math.log2(spam_count / spam_denominator)\n",
    "                probability_ham += math.log2(ham_count / ham_denominator)\n",
    "\n",
    "        # print(probability_spam, probability_ham)\n",
    "\n",
    "        # by the naive bayes algorithm , take maximum probability as prediction class\n",
    "        if probability_spam > probability_ham:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "\n",
    "        # result array has two dimensional arrays in it for each test sample\n",
    "        # test sample x = [ actual class, predicted class ]\n",
    "        results[i] = [x_test[i][1], predictions[i]]\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad0c90-a096-40ab-b7e4-b4c431241c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(x):\n",
    "    text_col_spam = []\n",
    "    text_col_ham = []\n",
    "    for i in range(x.shape[0]):\n",
    "        if x[i][1] == 1:\n",
    "            text_col_spam.append(x[i, 0])\n",
    "        elif x[i][1] == 0:\n",
    "            text_col_ham.append(x[i, 0])\n",
    "\n",
    "    count_vectorizer_spam = CountVectorizer()\n",
    "    matrix = count_vectorizer_spam.fit_transform(text_col_spam)\n",
    "    vocabulary_spam = count_vectorizer_spam.get_feature_names_out()\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_spam)), ('tfidf', TfidfTransformer())]).fit(\n",
    "        text_col_spam)\n",
    "\n",
    "    spam_tf_idf_arr = pipe['tfidf'].idf_\n",
    "    spam_tf_idf_dict = {}\n",
    "    for i in range(len(spam_tf_idf_arr)):\n",
    "        spam_tf_idf_dict[vocabulary_spam[i]] = spam_tf_idf_arr[i]\n",
    "\n",
    "    spam_tf_idf_arr = sorted(spam_tf_idf_arr)\n",
    "    spam_words = []\n",
    "    for i in range(100):\n",
    "        val = spam_tf_idf_arr[i]\n",
    "        for el in spam_tf_idf_dict.keys():\n",
    "            if spam_tf_idf_dict.get(el) == val:\n",
    "                spam_words.append(el)\n",
    "                break\n",
    "\n",
    "    # Ham part\n",
    "    count_vectorizer_ham = CountVectorizer()\n",
    "    matrix = count_vectorizer_ham.fit_transform(text_col_ham)\n",
    "    vocabulary_ham = count_vectorizer_ham.get_feature_names_out()\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary_ham)), ('tfidf', TfidfTransformer())]).fit(\n",
    "        text_col_ham)\n",
    "\n",
    "    ham_tf_idf_arr = pipe['tfidf'].idf_\n",
    "    ham_tf_idf_dict = {}\n",
    "    for i in range(len(ham_tf_idf_arr)):\n",
    "        ham_tf_idf_dict[vocabulary_ham[i]] = ham_tf_idf_arr[i]\n",
    "\n",
    "    ham_tf_idf_arr = sorted(ham_tf_idf_arr)\n",
    "    ham_words = []\n",
    "    for i in range(100):\n",
    "        val = ham_tf_idf_arr[i]\n",
    "        for el in ham_tf_idf_dict.keys():\n",
    "            if ham_tf_idf_dict.get(el) == val:\n",
    "                ham_words.append(el)\n",
    "                break\n",
    "\n",
    "    spam_words = list(dict.fromkeys(spam_words))\n",
    "    ham_words = list(dict.fromkeys(ham_words))\n",
    "    a, b = [spam_words, ham_words]\n",
    "    s = [x for x in b if x in a]\n",
    "    for i in range(len(s)):\n",
    "        word = s[i]\n",
    "        if (spam_tf_idf_dict.get(word) - ham_tf_idf_dict.get(word)) < 1.5:\n",
    "            if spam_words.count(word) > 0:\n",
    "                spam_words.remove(word)\n",
    "            if ham_words.count(word) > 0:\n",
    "                ham_words.remove(word)\n",
    "\n",
    "    print(\"spam words\", spam_words[:10])\n",
    "    print(\"ham words\", ham_words[:10])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb069b-02cb-4d6e-a8dc-17dcad08bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while calculating performance metrics\n",
    "# True positive = th -> truly predicted ham mail\n",
    "# False positive = fh -> falsely predicted ham mail\n",
    "# True negative = ts -> truly predicted spam mail\n",
    "# False negative = fs -> falsely predicted spam mail\n",
    "def calculate_performance(results):\n",
    "    th = 0\n",
    "    ts = 0\n",
    "    fh = 0\n",
    "    fs = 0\n",
    "    for key, value in results.items():\n",
    "        # if the mail is ham and predicted as ham\n",
    "        if value[0] == value[1] and value[1] == 0:\n",
    "            th += 1\n",
    "        # if the mail is spam and predicted as spam\n",
    "        elif value[0] == value[1] and value[1] == 1:\n",
    "            ts += 1\n",
    "        # if the mail is spam but predicted as ham\n",
    "        if value[0] != value[1] and value[1] == 0:\n",
    "            fh += 1\n",
    "        # if the mail is ham but predicted as spam\n",
    "        elif value[0] != value[1] and value[1] == 1:\n",
    "            fs += 1\n",
    "\n",
    "    accuracy = (th + ts) / (th + ts + fh + fs)\n",
    "    precision = th / (th + fh)\n",
    "    recall = th / (th + fs)\n",
    "    f1_score = (2 * recall * precision) / (recall + precision)\n",
    "    return accuracy, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bd3817d-6f9f-45c3-8c9b-5f80dffd9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(total_words_dis=None):\n",
    "    # reading data's in the csv file to the numpy array\n",
    "    df = pd.read_csv('./emails.csv')\n",
    "    x = np.array(df.iloc[:, :])\n",
    "\n",
    "    # shuffle the data\n",
    "    np.random.seed(101)\n",
    "    np.random.shuffle(x)\n",
    "    np.random.seed(102)\n",
    "    np.random.shuffle(x)\n",
    "    np.random.seed(103)\n",
    "    np.random.shuffle(x)\n",
    "\n",
    "    # split data %80 train - %20 test\n",
    "    x_test, x_train = split_data(x.copy())\n",
    "\n",
    "    # create dictionary of unique words\n",
    "    unique_words_dict, count_mails, total_words_dist = vectorizer(x_train.copy(), 1)\n",
    "\n",
    "    # PART1\n",
    "    max_prob_words_by_naive_bayes(unique_words_dict, total_words_dist)\n",
    "\n",
    "    # PART2\n",
    "    # calculate probabilities of all given test data\n",
    "    results = calculate_probability(x_test.copy(), unique_words_dict, count_mails, total_words_dist, 1)\n",
    "\n",
    "    # calculate performance of the given results\n",
    "    accuracy, precision, recall, f1_score = calculate_performance(results)\n",
    "    print(\"Unigram Accuracy: \", accuracy)\n",
    "    print(\"Unigram Precision: \", precision)\n",
    "    print(\"Unigram recall: \", recall)\n",
    "    print(\"Unigram F1 score: \", f1_score)\n",
    "    print()\n",
    "\n",
    "    # BIGRAM\n",
    "    # create dictionary of unique words\n",
    "    unique_words_dict, count_mails, total_words_dist = vectorizer(x_train.copy(), 2)\n",
    "\n",
    "    # PART1\n",
    "    max_prob_words_by_naive_bayes(unique_words_dict, total_words_dist)\n",
    "\n",
    "    # PART2\n",
    "    # calculate probabilities of all given test data\n",
    "    results = calculate_probability(x_test.copy(), unique_words_dict, count_mails, total_words_dist, 2)\n",
    "\n",
    "    # calculate performance of the given results\n",
    "    accuracy, precision, recall, f1_score = calculate_performance(results)\n",
    "    print(\"Bigram Accuracy: \", accuracy)\n",
    "    print(\"Bigram Precision: \", precision)\n",
    "    print(\"Bigram recall: \", recall)\n",
    "    print(\"Bigram F1 score: \", f1_score)\n",
    "    print()\n",
    "\n",
    "    # PART3\n",
    "    print(\"TF-IDF\")\n",
    "    tf_idf(x_train.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87919dbe-a838-4273-b815-150f72a8fcdf",
   "metadata": {},
   "source": [
    "## **1. Part 1: Understanding the data**\n",
    "\n",
    "\n",
    "For this part we implement a function named \"max_prob_words_by_naive_bayes\" (you can see in above code). In this function, we only consider frequencies of each word appeared in mails. Each words frequency calculated by count vectorizer whic is predefined in scikit learn library. By this vector we can calculate each words frequency in spam/ham mails. We calculate the frequencies for each word and make a dictionary with key value pair is shown as { word = [ spam count, ham count ] }. With those values we try to predict most usful keywords. When we try to figure out those keywords from our data, we try to select words which are not appeared often in both of the classes. If a word is appeared often in both classes, then this word has no effect on our prediction. \n",
    "\n",
    "\n",
    "\n",
    "With these values, we find 3 words which might be useful for us while predicting.\n",
    "* Most frequent ham word : &emsp; **vince**  &emsp;&emsp;&emsp;(appeared 2248 times in ham mails)\n",
    "* Second frequent ham word : &emsp;  **enron** &emsp;&emsp;(appeared 2081 times in ham mails)\n",
    "* Third frequent ham word : &emsp; **cc**   &emsp;&emsp;&emsp;&emsp;(appeared 1765 times in ham mails)\n",
    "\n",
    "\n",
    "\n",
    "* Most frequent spam word :  **here**  403\n",
    "* Second frequent soam word :  **click** 246\n",
    "* Third frequent spam word :  **000** 137\n",
    "\n",
    "So, as we can see some words appeared more than the others. And this makes that specific word a better indicator of spam/ham mail. As we can see with our eyes too, those words appeared very often in their classes.For example the word \"vince\" appeared 2248 time in ham mails but there arent any appearence in spam mails. So that makes \"vince\" a very great indicator of being ham mail. By using those values, we can implement our naive bayes algorithm. As we can see from the words, naive bayes algorithm is appropriate and applicable. While some words indicate being ham, others could indicate being spam. With naive bayes algorithm we can calculate the probability of both cases and make comparison between them. We think naive bayes algorithm is applicable, by looking at the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288459a-0dd4-4a4a-a1c1-98d034e1f440",
   "metadata": {},
   "source": [
    "## **2. Part 2: Implementing Naive Bayes**\n",
    "You will represent your data with listed approaches and use them to learn a classifier via Naive Bayes algorithm. You have to implement your own Naive Bayes\n",
    "algorithm.\n",
    "• Features: You will use Bag of Words (BoW) model which learns a vocabulary from all of the documents, then models each document by counting the\n",
    "number of times each word appears. You will use BoW with two options:\n",
    "– Unigram: The occurrences of words in a document(frequency of the\n",
    "word).\n",
    "– Bigram: The occurrences of two adjacent words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc0b2e-eaa1-414a-8dd2-7edd4aed5631",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705cd5cc-b70e-48e2-8170-c0a44446ad9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c584c41-0488-4c2e-b54d-156a3fae8027",
   "metadata": {},
   "source": [
    "## **3. Part 3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02f3fd-b013-4bbb-9739-da4187d63362",
   "metadata": {},
   "source": [
    "### **3.a) Analyzing effect of the words on prediction**\n",
    "• List the 10 words whose presence most strongly predicts that the mail is\n",
    "ham.\n",
    "\n",
    "\n",
    "• List the 10 words whose absence most strongly predicts that the mail is\n",
    "ham.\n",
    "\n",
    "\n",
    "• List the 10 words whose presence most strongly predicts that the mail is\n",
    "spam.\n",
    "\n",
    "\n",
    "• List the 10 words whose absence most strongly predicts that the mail is\n",
    "spam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c08e11-a5f6-4ada-ab8f-51cc9a79ba8d",
   "metadata": {},
   "source": [
    "### **3.b) Stopwords**\n",
    "You may find common words like a, to, and others in your list in Part 3(a).\n",
    "These are called stopwords. A list of stopwords is available in sklearn here.\n",
    "You can import this as follows:\n",
    "from sklearn.feature extraction.text import ENGLISH STOP WORDS\n",
    "Now, list the 10 non-stopwords that most strongly predict that the mail is\n",
    "ham, and the 10 non-stopwords that most strongly predict that the mail is\n",
    "spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd23ee-e66e-492e-b7f2-b46b77bae35f",
   "metadata": {},
   "source": [
    "### **3.c) Analyzing effect of the stopwords**\n",
    "Why might it make sense to remove stop words when interpreting the model?\n",
    "Why might it make sense to keep stop words?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417305e-8a85-4956-93d2-de65e8be2686",
   "metadata": {},
   "source": [
    "## **4. Part 4: Calculation of Performance Metrics**\n",
    "You will compute performance metrics below of your model to measure the success\n",
    "of your classification method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6841708-f62e-46a4-ac07-2a46ed2df1e3",
   "metadata": {},
   "source": [
    "### **Unigram**\n",
    "\n",
    "* Unigram Accuracy:  0.9921397379912664\n",
    "* Unigram Precision:  0.9964994165694282\n",
    "* Unigram recall:  0.9930232558139535\n",
    "* Unigram F1 score:  0.9947582993593477"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebbbe5-ee58-45d3-a2ed-1e2252e90166",
   "metadata": {},
   "source": [
    "### **Bigram**\n",
    "\n",
    "* Bigram Accuracy:  0.97117903930131\n",
    "* Bigram Precision:  0.9975932611311673\n",
    "* Bigram recall:  0.963953488372093\n",
    "* Bigram F1 score:  0.9804849201655825"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075e397-bfcb-4118-ad18-e5ea73df0ad8",
   "metadata": {},
   "source": [
    "### **TF-IDF Vectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc7086-718d-4b67-91ee-ea3cbfd73013",
   "metadata": {},
   "source": [
    "### **English Stop Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffee5b5-836a-45ad-a87f-7efe9f3a087f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
